{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3787ea19",
   "metadata": {},
   "source": [
    "# Assignment 1: Regular Expression & Tokenization (50 Points)\n",
    "\n",
    "Hello everyone, this assignment notebook covers **Regular Expressions** and **Tokenization**. There are some code-completion tasks in this notebook. For each code completion task, please write down your answer (i.e., your lines of code) between sentences that `Your code starts here` and `Your code ends here`. The space between these two lines does not reflect the required or expected lines of code.\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:**\n",
    "* Remember to rename and save this Jupyter notebook as **A1_YourName_YourNUSNETID.ipynb** (e.g., **A1_BobSmith_e12345678.ipynb**) before submission! Failure to do so will yield a penalty of 1 Point.\n",
    "* Remember to rename and save the script file **A1.py** as **A1_YourName_YourNUSNETID.py** (e.g., **A1_BobSmith_e12345678.py**) before submission! Failure to do so will yield a penalty of 1 Point.\n",
    "* Submission deadline is Feb 7, 12 noon. Late submissions will be penalized by 10% for each additional day.\n",
    "* **Do not change any predefined variable names!** For example in Task 1.1 a), you need to find the correct RegEx as value for the variable named `pattern_11a`; do not change this variable name!\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f38b136-64a1-4f09-8227-fbeb6585e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = 'A0286188L'\n",
    "nusnet_id = 'E1237250'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415bef3a",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts are there to provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well marked, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 Regular Expressions (25 Points)**\n",
    "    * 1.1 Basic Expressions (15 Points)\n",
    "        * 1.1 a) RegEx 1 (3 Points)\n",
    "        * 1.1 b) RegEx 2 (3 Points)\n",
    "        * 1.1 c) RegEx 3 (3 Points)\n",
    "        * 1.1 d) RegEx 4 (3 Points)\n",
    "        * 1.1 e) RegEx 5 (3 Points)\n",
    "    * 1.2 Regular Expressions & Finite State Automata (10 Points)\n",
    "        * 1.2 a) FSA 1 (2 Points)\n",
    "        * 1.2 b) FSA 2 (2 Points)\n",
    "        * 1.2 c) FSA 3 (2 Points)\n",
    "        * 1.2 d) FSA 4 (2 Points)\n",
    "        * 1.2 e) FSA 5 (2 Points)        \n",
    "* **2 Tokenization (25 Points)**\n",
    "    * 3.1 Implementation of a Basic Tokenizer (15 Points)\n",
    "    * 3.2 Additional Questions (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c454e2",
   "metadata": {},
   "source": [
    "### Notebook Setup\n",
    "\n",
    "The following code cell ensures that the .py file gets automatically reloaded after you save any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c499d3-902d-422c-bfb7-f437546a3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481d54b",
   "metadata": {},
   "source": [
    "#### Required Imports\n",
    "\n",
    "For this notebook, we only need the 2 in-built packages `re` and `json` (in fact, the latter is only used for printing some outputs more nicely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb63e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e88177-794f-4051-9b4d-a0924156bef1",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `A1.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b8c3d-0710-4b0a-b0e3-c56b1d10942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from A1 import MyTokenizer\n",
    "#from A1_BobSmith_e12345678 import MyTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792cd20-cfd8-4895-8543-7f2f16d2e13c",
   "metadata": {},
   "source": [
    "#### Utility Method\n",
    "\n",
    "The method `show_matches()` offers a very simple way to print the result of a substring matching using a Regular Expression. For a given input text and RegEx pattern it returns all matches separated by the pipe symbol (`|`) a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ca0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_matches(pattern, string, flags=0):\n",
    "    # Compile RegEx pattern\n",
    "    p = re.compile(pattern, flags=flags)\n",
    "    # Match pattern against input text\n",
    "    matches = list(p.finditer(string))\n",
    "    # Handle matches\n",
    "    if len(matches) == 0:\n",
    "        print(\"No match found.\", \"\\n\")\n",
    "    else:\n",
    "        print(' | '.join([ m.group() for m in matches] ), '\\n')\n",
    "            \n",
    "show_matches(r\"[\\w]+\", \"Welcome to the assignment for Section 2 of NLP Foundations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ae408",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f7236",
   "metadata": {},
   "source": [
    "## 1 Regular Expressions\n",
    "\n",
    "### 1.1 Basic Expressions\n",
    "\n",
    "In each of the following 5 subtasks, you will need to write a Regular Expression to match the substrings as described in the subtask. To keep things simple, wherever it might be a concern, **the case of letters does not matter**. In other words, you can always assume that your Regular Expression gets evaluated with the flag `re.IGNORECASE`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f2d057",
   "metadata": {},
   "source": [
    "#### 1.1 a) Match SoC Course Codes (3 Points)\n",
    "\n",
    "We want to scrape the Web to see if any website is offering course materials offered by SoC; see [this page for a complete course list](https://www.comp.nus.edu.sg/cug/soc-sched/). To do this, we crawl the web page and check the content if it contains any course code. A course code contains (a) 2-3 letters, (b) 4 digits, and (c) and optional letter. To be a bit more flexible, we allow an optional whitespace between these 3 components. In short:\n",
    "\n",
    "* We want to match: CS5246, CS 5246, IFS4101, CS1010E, CS1010 E, CS 1010E, etc.\n",
    "* We do NOT want to match: anything else, incl. invalid letter codes (e.g., XX5246)\n",
    "\n",
    "**Important** (to keep it simple)\n",
    "\n",
    "* You only need to check if the first 2-3 letters mark an existing course; the 4-digits and the optional letter do not need to refer to an existing course. So matching CS9999 is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d55c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "pattern_11a = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   \n",
    "\n",
    "show_matches(pattern_11a, \"You can download the slides for CS 5246 here. The password is XX5246.\", flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb393f5c",
   "metadata": {},
   "source": [
    "The expected output for the code cell above is:\n",
    "\n",
    "```\n",
    "CS 5246\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669223df-19f6-4212-ab08-baaaf4f31745",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.1 b) Match NUS Phone Numbers (3 Points)\n",
    "\n",
    "Similar to 1.1 c), we want to know which websites list **NUS phone numbers**. According to the national conventions, phone numbers in Singapore have the format `+65-XXXX-YYYY` or `+65 XXXX YYYY` -- however we also want to allow `+65XXXXYYYY` as well as making the country codes optional. Since we are only interested in NUS phone numbers, `XXXX=6516`. In short:\n",
    "\n",
    "* We want to match: +65 6516 1234, +65-6516-1234, +6565161234, 6516 1234, 6516-1234, etc.\n",
    "* We do NOT want to match: anything else, e.g., +65-6516-12345678, +65-1111-1234 (not NUS!)\n",
    "\n",
    "**Important** (to keep it simple)\n",
    "\n",
    "* You don't have to worry if the separator is used consistently (e.g. +65-6516-1234, +656516-1234, +65 65161234 are all fine to match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ac619-5f42-4599-be0e-01f3937a4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "pattern_11b = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   \n",
    "\n",
    "show_matches(pattern_11b, \"You can reach me at +65 6516-1234.\", flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76fd71-6784-4e89-82d4-2bdd769cf10a",
   "metadata": {},
   "source": [
    "The expected output for the code cell above is:\n",
    "\n",
    "```\n",
    "+65 6516-1234\n",
    "```\n",
    "\n",
    "Note that we would match any 8 digit number starting with `6516`. For example in *\"My highscore is 6516300 points\"*, we will match the `6516300`. This cannot be avoided using simple substring matching and would require some deeper analysis which is beyond our scope here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bce31",
   "metadata": {},
   "source": [
    "#### 1.1 c) Match HDB Postal Addresses (3 Points)\n",
    "\n",
    "You are looking to by an **HDB flat** in the **Jurong area**. To make the best decision, you are trying to collect as much information about potential flats as possible. As a computer scientist and expert text miner, you know you can find the information from text document (e.g., crawled from websites) by match\n",
    "relevant postal addresses.\n",
    "\n",
    "The general format for postal addresses in Singapore you can find [here](https://en.wikipedia.org/wiki/Postal_codes_in_Singapore) (6-digit postal code). You also want to utilize the fact that for HDBs, the block number is the same as the last 3-digits of the postal code. For example, if the postal code is 600115, the block number is 115. The address may or may not start with \"BLOCK\" or \"BLK\", so this should be optional. Lastly, recall that we are only interested in addresses in the Jurong area; [this](https://www.propertyguru.com.sg/property-guides/postal-codes-in-singapore-10567) website might help with that.\n",
    "\n",
    "* We want to match: `BLK 115 Jurong East Street 13 Singapore 600115`, `115 Pioneer Road North 5 Singapore 600115`, `115A Jalan Boon Lay 12 Singapore 600115`\n",
    "* We do NOT want to match, e.g.: `BLK 150A Bishan Street 11 Singapore 571150` (not in Jurong area), `BLK 115 Jurong East Street 13 Singapore 600116` (block number does not match last 3 digits of postal code)\n",
    "\n",
    "**Important** (to keep it simple)\n",
    "\n",
    "* You can assume that there are no line breaks; you can always replace line break characters with whitespace characters.\n",
    "* You can assume that there are no commas; you can always remove commas or replace them with whitespace characters.\n",
    "* You need to care about the street name and street number (e.g., Jurong East Street 13), but \"Singapore\" should be stated before the postal code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed52602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "pattern_11c = r\"\"\n",
    "  \n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   \n",
    "\n",
    "show_matches(pattern_11c, \"Around 115 Jurong East Street 13 Singapore 600115 one can find the most number of 2-bedroom flats.\", flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f3783",
   "metadata": {},
   "source": [
    "The expected output for the code cell above is:\n",
    "\n",
    "```\n",
    "115 Jurong East Street 13 Singapore 600115 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2bc18",
   "metadata": {},
   "source": [
    "#### 1.1 d) Filtering Apache Access Logs\n",
    "\n",
    "Apache logs refer to the log files generated by the Apache HTTP Server, which is one of the most widely used web servers. These logs provide a record of events and transactions that occur on the server. The Apache server logs capture various types of information about requests and responses, server status, errors, and more. These logs are commonly used for debugging and troubleshooting, security monitoring, performance analysis, audit trails, and user behavior analysis.\n",
    "\n",
    "In the following, we use a small sample from the publicly available [NASA Apache Log](https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html) dataset. The format of the log files is [standardized](https://httpd.apache.org/docs/2.4/logs.html) with each line having a fixed structure, but you can also simply have a look into the file itself.\n",
    "\n",
    "**Task:** Find all log entries -- a log entry is a single line in the log file -- that indicate POST request from a machine without a proper domain name (i.e., IP address only) that result on [status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) that was not 200 (meaning: OK). I have to solve this task using a single Regular Expression!\n",
    "\n",
    "**Important** (to keep it simple):\n",
    "\n",
    "* You don't have to check if a domain name (e.g., hello.world.com) or IP address (999.99.9.999) is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f93357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "pattern_11d = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   \n",
    "\n",
    "\n",
    "# Since we want the while entry/line, and not just the match, we cannot use the method show_matches() here.\n",
    "# We simple check if the RegEx matches, and print the line if this is indeed the case.\n",
    "with open(\"data/nasa-access-sample.txt\") as file:\n",
    "    for line in file:    \n",
    "        line = line.strip()\n",
    "        # Match pattern against input text\n",
    "        match = re.search(pattern_11d, line, flags=re.IGNORECASE)\n",
    "        if match is not None:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86adf64",
   "metadata": {},
   "source": [
    "The expected output for the code cell above is:\n",
    "\n",
    "```\n",
    "133.43.106.47 - - [01/Aug/1995:00:24:29 -0400] \"POST /images/ksclogo-medium.gif HTTP/1.0\" 300 5866\n",
    "198.248.59.123 - - [01/Aug/1995:00:26:25 -0400] \"POST /icons/menu.xbm HTTP/1.0\" 304 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195885f-e15f-4f68-89d2-8876174f992e",
   "metadata": {},
   "source": [
    "#### 1.1 e) Singapore License Plate Numbers (Private Cars Only!)\n",
    "\n",
    "Singapore license plate numbers follow a well-defined but quite complex scheme; fully detailed on this [Wikipedia article](https://en.wikipedia.org/wiki/Vehicle_registration_plates_of_Singapore). We want to match all license plate numbers but only for private cars. This excludes taxis, busses, or anything else listed under *\"Other specific vehicle types\"* in the article. In short:\n",
    "\n",
    "* We want to match: SGM1234A, SM1234A, SGM123A, etc.\n",
    "* We do NOT want to match: anything else, e.g., SBS1234A (bus), SHA1234A (taxi), SEP1 (elected president)\n",
    "\n",
    "Check the [Wikipedia article](https://en.wikipedia.org/wiki/Vehicle_registration_plates_of_Singapore) for the complete list of license plate numbers we do NOT want to match.\n",
    "\n",
    "**Important** (to keep it simple):\n",
    "\n",
    "* The last letter is actually a checksum letter, but you do not have to check the checksum works out.\n",
    "* You can assume that there are no whitespace anywhere between the characters of the license plate number.\n",
    "* You can assume that all license place numbers of interest start with 3 letters (although 2 might occur in reality).\n",
    "* You can ignore the rule: *Alphabetical series (\"I\" and \"O\" are not used to avoid confusion with \"1\" and \"0\")* (cf. [Wikipedia article](https://en.wikipedia.org/wiki/Vehicle_registration_plates_of_Singapore))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620dce01-77e9-483c-b973-13874b619356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "pattern_11e = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   \n",
    "\n",
    "show_matches(pattern_11e, \"I thought I saw the license plate SBS1234A on a car today, but it was SBF1234A in the end\", flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d94d53-14b6-4c30-8ac5-24e74198b413",
   "metadata": {},
   "source": [
    "The expected output for the code cell above is:\n",
    "\n",
    "```\n",
    "SBF1234A\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018ea88",
   "metadata": {},
   "source": [
    "### 1.2 Regular Expressions & Finite State Automata (10 Points)\n",
    "\n",
    "We saw that a Regular Expression describe **Regular Language** which is a language accepted by a Finite State Automaton (FSA). This means that each Regular Expression can be represented by an FSA, and vice versa. In fact, this mapping might not even be unique -- that is, the same Regular Expression can be represented by different FSA and vice versa. In short, there is generally no unique solution for all subtasks below.\n",
    "\n",
    "In the following, you are given 4 FSA, and your task is to find a Regular Expression that matches each FSA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbe6a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.2 a) Given is the Finite State Automata below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b71aa0-32bb-4450-93e9-5b1e1e0631a7",
   "metadata": {},
   "source": [
    "<img src=\"data/a1-fsa-01.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bb9ba",
   "metadata": {},
   "source": [
    "Find a Regular Expression that matches the FSA above and enter it in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "regex_12a = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c365cbf4",
   "metadata": {},
   "source": [
    "#### 1.2 b) Given is the FSA below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc69ef-b018-49be-bfd7-04ff1b4d9226",
   "metadata": {},
   "source": [
    "<img src=\"data/a1-fsa-02.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cedf8",
   "metadata": {},
   "source": [
    "Find a Regular Expression that matches the FSA above and enter it in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "regex_12b = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73adcae-4819-4bdd-a83f-5c1cbd7a0897",
   "metadata": {},
   "source": [
    "#### 1.2 c) Given is the FSA below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98443f1a-7ba6-4ee7-9802-ae54a9d6a3c2",
   "metadata": {},
   "source": [
    "<img src=\"data/a1-fsa-03.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875efca3-897b-4816-8f88-79577e1f8dbf",
   "metadata": {},
   "source": [
    "Find a Regular Expression that matches the FSA above and enter it in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9db15-e2d7-4c5b-8c0f-45cf8a892c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "regex_12c = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cb2da-ce7a-413a-a826-94a3c1fa6bf7",
   "metadata": {},
   "source": [
    "#### 1.2 d) Given is the FSA below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c92aa9-d80e-4222-b2e9-c9871f7eb490",
   "metadata": {},
   "source": [
    "<img src=\"data/a1-fsa-04.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e48ee-06ae-4fc1-9c65-c823cce24e07",
   "metadata": {},
   "source": [
    "Find a Regular Expression that matches the FSA above and enter it in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892872ea-e4eb-4fd4-bf82-ca99f9571dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "regex_12d = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24089b-2f0c-425e-90f1-2c12ece1d8f4",
   "metadata": {},
   "source": [
    "#### 1.2 e) Given is the FSA below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b7ef5-7c97-4c67-a7cb-af5e06bb7434",
   "metadata": {},
   "source": [
    "<img src=\"data/a1-fsa-05.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1c893-85be-4219-8310-04b1e2582cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "regex_12e = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f5678",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389e1e0",
   "metadata": {},
   "source": [
    "## 2 Tokenization (25 Points)\n",
    "\n",
    "We covered a series of common preprocessing steps performed over raw text. Many of them are very simple to implement. For example, Python (but arguably all modern programming languages) come with in-built methods to lowercase or uppercase a string; see the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a TEST to see how SiMpLe case-folding a STRING is.\".lower()\n",
    "#text = \"This is a TEST to see how SiMpLe case-folding a STRING is.\".upper()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc800144",
   "metadata": {},
   "source": [
    "Another common and easy-to-implement preprocessing step is stopword removal. In the code cell below, we make 2 assumptions\n",
    "\n",
    "* We have a list of stopwords we want to remove (such list are widely available, and part of many NLP toolkits).\n",
    "* Our text has already been tokenized -- we address the challenge of tokenization in more detail below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined set of stopwords (very simplified) -- note that all are lowercase\n",
    "stopwords = [\"a\", \"an\", \"the\", \"and\", \"but\", \"to\", \"be\", \"is\", \"'s'\", \"was\", \"not\", \"i\", \"me\", \"my\"]\n",
    "\n",
    "# Tokenized input text\n",
    "tokens = [\"I\", \"like\", \"to\", \"study\", \"NLP\", \".\"]\n",
    "\n",
    "# Remove stopwords using list comprehension\n",
    "tokens = [ t for t in tokens if t.lower() not in stopwords ]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e43095",
   "metadata": {},
   "source": [
    "Since case-folding and stopword removal does not pose much of a challenge, we don't ask you to implement these steps.\n",
    "\n",
    "In contrast, we saw that other common preprocessing steps are much less straightforward, particularly stemming and lemmatization. While rule and lookup-based approaches are conceptually not very difficult, their actual implementation requires a certain amount of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ab3e8",
   "metadata": {},
   "source": [
    "## 2.1 Implementation of a Basic Tokenizer (15 Points)\n",
    "\n",
    "We saw that tokenization is one of the most important preprocessing steps. The goal of tokenization is to split a string into tokens, where a token is a sequence of characters with a semantic meaning (typically: words, numbers, punctuation -- but may differ depending on the application).\n",
    "\n",
    "Your task is to implement a simple but still practical tokenizer. There are different approaches to this task. For this assignment, we follow the basic 2-part approach of the spaCy tokenizer as already covered on our slides:\n",
    "\n",
    "* Split the input string w.r.t. whitespace characters (to get the initial set of tokens).\n",
    "* From left to right, recursively check each token if it should be split into further subtokens.\n",
    "\n",
    "The screenshot taken from the [spaCy website](https://spacy.io/usage/spacy-101#annotations-token) illustrates the idea:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b546e3b",
   "metadata": {},
   "source": [
    "<img src=\"data/spacy-tokenizer-example.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f6e06",
   "metadata": {},
   "source": [
    "As you can see from the illustration, if a token gets split, the resulting subtokens have to be checked again until no further splitting is possible. Only then the tokenizer can move on to the current token list. The method `tokenize()` already implements this idea, and you don't have to worry about that.\n",
    "\n",
    "The heart of this method is the call of method `split_token()` which checks if a token needs to be split or not (e.g., *Let's* into *Let* and *'s*). If indeed a split is required, `split_token()` splits the token into 2 or more subtokens -- that's kind of up to you -- and returns the new subtokens as a list.\n",
    "\n",
    "Have a good look at method `tokenize()` and convince yourself that it implements the basic approach of the spaCy tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb379cb0",
   "metadata": {},
   "source": [
    "Your task is now to implement the method `split_token()` in the code below. The method takes a token as input -- note that a token can never contain any whitespace character -- and decides if the token needs to be split according to the given tokenization rules. There a variety of such rules -- and different off-the-shelf tokenizers may differ in their set of rules -- but for this task focus only on the following:\n",
    "\n",
    "* **Basic punctuation:** The most common limitation of tokenizing only using whitespace characters is that there is no whitespace between a word token and a common punctuation mark. So we need to split this. For this, we provide you with the solution -- see the `split_token()` method below -- to illustrate how to use the method and what needs to be returned.\n",
    "* **Abbreviations:** Abbreviations such as *\"N.Y.\"* (cf. above) or *\"U.S.A.\"* should be treated as a complete token. Therefore the rule is to *not* split abbreviations. For simplification, you can always assume that an abbreviations contains at least 2 abbreviation dots/periods -- that is, something like *\"I saw A. this morning\"* (where *A.* might stand for \"Alice\") does not occur. Hint: Note that this rule and the one for handling basic punctuation marks are connected.\n",
    "* **Ellipsis:** In informal writing, an ellipsis (`...`) can be used to represent a trailing off of thought (e.g., *\"If only she had . . . Oh, it doesn’t matter now.\"*) or also indicate hesitation (e.g., *\"I wasn’t really . . . well, what I mean . . . see, the thing is . . . I didn’t mean it.\"*), though in this case the punctuation is more accurately described as suspension points. You can assume that each ellipsis is correctly represented by 3 periods (`...`), no more no less. While some style guides suggest having a whitespace character between words and `...`, these whitespace characters are not mandatory and often omitted. So we have to take this into consideration.\n",
    "* **Clitics:** A clitic is a morpheme that has syntactic characteristics of a word, but depends phonologically on another word or phrase. A clitic is pronounced like an affix, but plays a syntactic role at the phrase level. For example, the contracted forms of the auxiliary verbs in *I'm* and *we've* are clitics. The negative marker *n't* as in *couldn't* etc. is typically considered a clitic. In line with most tokenizers, we also want to split clitics from the preceding word.\n",
    "* **Hyphenated words:** Most tokenizers (incl. the spaCy tokenizer) split hyphenated words such as *long-lived*, *old-fashioned*, or *mother-in-law*. Hence, our tokenizer should do the same.\n",
    "\n",
    "\n",
    "**Comment & Hints**\n",
    "* You can assume that the text contains only basic [ASCII Characters](https://www.w3schools.com/charsets/ref_html_ascii.asp); no Unicode or anything else. In short, just the characters you find on an English keyboard.\n",
    "* All rules can be implemented using Regular Expression in a rather straightforward manner. However, you are **not required** to use Regular Expressions for this task. For example, you can look into basic methods provided by Python such as [`split()`](https://www.w3schools.com/python/ref_string_split.asp), [`startswidth()`](https://www.w3schools.com/python/ref_string_startswith.asp) or [`endswith()`](https://www.w3schools.com/python/ref_string_endswith.asp).\n",
    "* If you are using Regular Expressions, you may want to use groups to match the components that will be the result of the split, as this allows you to directly access the match groups and return them as the split components.\n",
    "* Once any rule requires the split of a token, you can immediately return `components`. There is no need to check the other rules, as the loop in `tokenize()` takes care that all new subtokens get checked again if a further splitting might be required.\n",
    "* To get you some ideas how to get started, we give you the snippet for a dummy rule. This dummy rule splits all tokens that contain the same sequence of characters twice (e.g., *\"bonbon\"*, or *\"papa\"*). Of course, in practice, we wouldn't split tokens this way. This is purely to provide you with an example of how you might want to approach this task. You can handle all the rules you need to implement in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fdaac-1ebf-402f-a3a6-70ba81657f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = MyTokenizer()\n",
    "\n",
    "my_tokenizer.tokenize(\"It's a free-for-all market...you should've invested! Too bad. Let's go to N.Y.! I want a bonbon!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f03630-e220-42f8-aa75-a8cd1a52d5d3",
   "metadata": {},
   "source": [
    "For the given example text, the expected outcome is as follows:\n",
    "\n",
    "```\n",
    "['It',\n",
    " \"'s\",\n",
    " 'a',\n",
    " 'free',\n",
    " '-',\n",
    " 'for',\n",
    " '-',\n",
    " 'all',\n",
    " 'market',\n",
    " '...',\n",
    " 'you',\n",
    " 'should',\n",
    " \"'ve\",\n",
    " 'invested',\n",
    " '!',\n",
    " 'Too',\n",
    " 'bad',\n",
    " '.',\n",
    " 'Let',\n",
    " \"'s\",\n",
    " 'go',\n",
    " 'to',\n",
    " 'N.Y.',\n",
    " '!',\n",
    " 'I',\n",
    " 'want',\n",
    " 'a',\n",
    " 'bon',\n",
    " 'bon',\n",
    " '!']\n",
    "```\n",
    "\n",
    "Again, the split of *\"bonbon\"* is simply because of the example snippet we provide. No proper word-based tokenizer would do this...maybe a subword-based one :)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab3c81e",
   "metadata": {},
   "source": [
    "### 2.2 Additional Questions (10 Points)\n",
    "\n",
    "Your implementation of `MyTokenizer` is of course not ready for production. There are arguably a variety of instances/rules, where a further splitting of tokens would be required. Outline **at least 5** (important) splitting rules are missing from your tokenizer implementation, and *briefly* discuss how a missing rule can be implemented and/or which challenge it might pose.\n",
    "\n",
    "**Comment & Hints**\n",
    "* There is nothing for you to implement here! Your explanations should be concise and to the point. 1-3 sentences should do.\n",
    "* We focus on tokenizing English sentences, so other languages are not relevant here and won't count :).\n",
    "\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f502d-1de4-4b17-aacd-abaf41685836",
   "metadata": {},
   "source": [
    "| Missing Type of Splitting Rule | Brief sketch for a solution and/or potential challenges |\n",
    "| --- | --- |  \n",
    "| ??? | ??? |\n",
    "| ??? | ??? |\n",
    "| ??? | ??? |\n",
    "| ??? | ??? |\n",
    "| ??? | ??? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a803241-bed8-4b70-a0a3-f1228cdd27fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e54f8-b34c-4750-87ee-44beb267c25b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This assignment tested you on working with strings, which represent the basic representation of text data. One the hand, basic NLP tasks such as substring matching using Regular Expression are a very useful tool in practice. On the other hand, for more sophisticated NLP tasks, we typically need to convert a text as a sequence of characters into meaningful units (typically words). Again, this can be done using Regular Expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b88c87-92a1-4285-a313-e9d2f052d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
